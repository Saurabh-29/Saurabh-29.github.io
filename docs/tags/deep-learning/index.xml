<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning | Academic</title>
    <link>https://saurabh-29.github.io/site/tags/deep-learning/</link>
      <atom:link href="https://saurabh-29.github.io/site/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Deep Learning</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 01 Sep 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://saurabh-29.github.io/site/img/icon-192.png</url>
      <title>Deep Learning</title>
      <link>https://saurabh-29.github.io/site/tags/deep-learning/</link>
    </image>
    
    <item>
      <title>Waveglow Inference</title>
      <link>https://saurabh-29.github.io/site/project/waveglow-inference/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://saurabh-29.github.io/site/project/waveglow-inference/</guid>
      <description>&lt;p&gt;Waveglow is a flow based model which converts noise to speech conditioned on mel-spectrograms through series of transformations. This project runs the Waveglow inference in CUDA C++ directly and gives speed-boost. It can also be used as a template for future CUDA Codes. More details are in this
&lt;a href=&#34;https://github.com/Saurabh-29/Waveglow_Inference_in_CUDA&#34; target=&#34;_blank&#34;&gt;repo&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Federated Learning</title>
      <link>https://saurabh-29.github.io/site/project/federated-learning/</link>
      <pubDate>Thu, 27 Jul 2017 00:00:00 +0000</pubDate>
      <guid>https://saurabh-29.github.io/site/project/federated-learning/</guid>
      <description>&lt;p&gt;The project solved the problem of L1 regularized regression task to reduce latency under different communication protocols when data is distributed across multiple heterogeneous devices i.e. in Federated Setting. Data is distributed across devices, with no central copy, model is trained on edge devices and parameters are shared across different devices through servers. Different compute capability, network capability and asynchronous nature make the problem challenging. We proposed &lt;strong&gt;FADMM: A faster approach to federated optimization&lt;/strong&gt; which achieved faster convergence than baselines. You can find the detailed report &lt;a href=&#34;https://docs.google.com/document/d/160m1Udb0F0XXdvRwF-kjr9d6KSmkjjc0nTSte7gdddE/edit?usp=sharing&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Speech Synthesis</title>
      <link>https://saurabh-29.github.io/site/project/text-2-speech/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://saurabh-29.github.io/site/project/text-2-speech/</guid>
      <description>&lt;p&gt;I have been working on this project for last 1 year. This project dealt with the problem of automated Speech Synthesis for Indian Languages that resembles, as closely as possible, a native speaker of the language. I was involved end-to-end in this project. It comprised of various steps such as:&lt;/p&gt;

&lt;p&gt;1) &lt;strong&gt;Data Collection&lt;/strong&gt;- Collection data from various sources such as audio-books, studio recording. We also used some open-source datasets like LjSpeech, VCTK etc.&lt;/p&gt;

&lt;p&gt;2) &lt;strong&gt;Data Cleaning&lt;/strong&gt;- Cleaning data to get the non-noisy audio-text aligned pairs&lt;/p&gt;

&lt;p&gt;3) &lt;strong&gt;Training Models&lt;/strong&gt;- It involved working on models like Tacotron, Voiceloop for text to mel systems and working on models for Wavenet, Parellel Wavenet, Clarinet and Waveglow for mel to speech systems.&lt;/p&gt;

&lt;p&gt;4) &lt;strong&gt;CUDA Inference&lt;/strong&gt;- Write the CUDA inference of models for faster inference.&lt;/p&gt;

&lt;p&gt;The project also involved working on some sub-projects to improve the overall MOS(Mean Opinion Score) such as:&lt;/p&gt;

&lt;p&gt;1) &lt;strong&gt;Prosody Transfer&lt;/strong&gt;-  For making the speech more prosodic(natural)&lt;/p&gt;

&lt;p&gt;2) &lt;strong&gt;Multi-Speaker TTS&lt;/strong&gt;-  To add multi-speaker support in our model&lt;/p&gt;

&lt;p&gt;3) &lt;strong&gt;Speaker-Separation&lt;/strong&gt;-  For separating dominant speakers from other spakers, used in data collection&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
